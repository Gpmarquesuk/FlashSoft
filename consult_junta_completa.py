"""
CONSULTA COMPLETA √Ä JUNTA DE ESPECIALISTAS
Modelos: Gemini 2.5 Pro, GPT-5 Codex, Grok 4

Cada modelo receber√° contexto completo do projeto FlashSoft e dever√°:
1. Analisar a arquitetura atual da f√°brica
2. Propor melhorias no workflow
3. Criticar problemas no modelo atual
4. Sugerir estrat√©gias de melhoria
5. Opinar sobre cria√ß√£o de UI para a f√°brica
"""

import os
import json
import time
from datetime import datetime
from llm_client import chat

# Carregar API key
api_key = os.getenv("OPENROUTER_API_KEY")
if not api_key:
    from dotenv import load_dotenv
    load_dotenv()
    api_key = os.getenv("OPENROUTER_API_KEY")

if not api_key:
    raise RuntimeError("OPENROUTER_API_KEY n√£o encontrada!")

print(f"‚úì API Key carregada: {api_key[:20]}...")

# Definir modelos EXATOS conforme solicitado
MODELS = {
    'gemini25pro': {
        'id': 'google/gemini-2.5-pro',
        'name': 'Gemini 2.5 PRO'
    },
    'gpt5codex': {
        'id': 'openai/gpt-5-codex',
        'name': 'GPT-5 CODEX'
    },
    'grok4': {
        'id': 'x-ai/grok-4',
        'name': 'Grok 4'
    }
}

# Contexto COMPLETO do projeto
CONTEXT = """
# CONTEXTO COMPLETO - F√ÅBRICA DE SOFTWARE FLASHSOFT

## OBJETIVO
F√°brica automatizada de software usando agentes de IA que transforma especifica√ß√µes YAML em Pull Requests funcionais no GitHub, com testes validados.

## ARQUITETURA ATUAL

### Pipeline Principal
1. **run_spec.ps1** ‚Üí Orchestrator Sentry ‚Üí **run.py** ‚Üí Agentes Sequenciais
2. Workspace tempor√°rio: /tmp/autobot_work (clone do repo)
3. Logs estruturados: run-*.jsonl (eventos), ORCH_*_out.txt (stdout/stderr)
4. Comit√™ de modelos LLM via OpenRouter API

### Agentes na Sequ√™ncia
1. **Task Decomposer**: Quebra spec em tarefas menores (opcional, desabilitado por padr√£o)
2. **Architect**: Analisa arquitetura e prop√µe estrutura de arquivos (opcional)
3. **Planner/Coder**: Gera patches (arquivos completos) em JSON: {"patches": [...], "test_plan": [...]}
4. **Tester**: Executa pytest e valida c√≥digo
5. **QA**: Verifica cobertura de testes
6. **Reviewer**: Faz code review e gera REVIEW.md
7. **Release**: Cria PR no GitHub

### Router System (llm_client.py)
- **Committee Pattern**: Cada agente tem lista de modelos fallback
- **Fallback Autom√°tico**: Se modelo falha, tenta pr√≥ximo da lista
- **force_json**: Pode for√ßar JSON mode ou permitir markdown

### Orchestrator Sentry (orchestrator_sentry.py)
- **Retry Logic**: At√© 6 tentativas com estrat√©gias adaptativas
- **Decis√µes Autom√°ticas**:
  - action=retry: Tenta novamente
  - action=force_json: For√ßa JSON mode no pr√≥ximo
  - action=switch_planner: Muda modelo do Planner
  - action=new_branch_suffix: Gera novo nome de branch
- **An√°lise de Logs**: Parser inteligente detecta tipo de erro

## PROBLEMAS CR√çTICOS ATUAIS

### 1. PLANNER FALHA 100% (at√© hoje)
- **Sintoma**: "Todas as estrat√©gias de sanitiza√ß√£o falharam"
- **Root Cause Identificado**: Context Window Overflow (GPT-5 Codex analysis)
- **Tentativas**: 10 retries esgotados, 5 estrat√©gias de JSON sanitizer
- **Modelos tentados**: Claude Sonnet 4.5, Gemini 2.5 Pro, GPT-4o

### 2. JSON SANITIZER (json_sanitizer.py)
Implementado com 5 estrat√©gias progressivas:
1. Direct parse
2. Markdown extraction (```json blocks)
3. Common error fixes (trailing commas, quotes)
4. Fuzzy extraction (balanced braces)
5. Structural repair (aggressive cleanup)

**Resultado**: Funciona em testes isolados, falha na pipeline real

### 3. CORRE√á√ïES RECENTES
- ‚úì run.py restaurado de backup (estava corrompido)
- ‚úì .env corrigido (linha breaks)
- ‚úì Prompts re-encodados UTF-8 (eram Latin-1)
- ‚úì Spec truncado em linhas completas (n√£o no meio do YAML)
- ‚úì Lista de arquivos reduzida 60‚Üí3
- ‚ö†Ô∏è Spec simples (hello-world) ‚Üí Planner funcionou MAS formato incorreto (falta "content")

### 4. √öLTIMO SUCESSO PARCIAL
Execu√ß√£o com spec hello-world:
- ‚úì Planner gerou JSON (primeira vez!)
- ‚úì planner_coder_done com success=true
- ‚úó KeyError: 'content' no patcher
- **Conclus√£o**: Formato JSON diferente do esperado

## PROMPTS DOS AGENTES

### planner_coder.md (Principal Problem√°tico)
Solicita JSON:
```json
{
  "patches": [
    {"path": "relative/path.py", "content": "arquivo inteiro"}
  ],
  "test_plan": [...]
}
```

Mas recebe formato DESCONHECIDO sem chave "content".

## TECNOLOGIAS
- Python 3.12.10, .venv
- OpenRouter API (sk-or-v1-8106...)
- GitHub CLI (gh 2.81.0)
- Git branching autom√°tico
- pytest para valida√ß√£o

## HIST√ìRICO
- 2+ semanas de desenvolvimento
- 20+ tentativas de execu√ß√£o TODAS falharam
- M√∫ltiplos agentes (GPT-5, Grok, Gemini, Claude) tentaram resolver
- Usu√°rio extremamente frustrado com falhas consecutivas
"""

# Perguntas para a junta
QUESTIONS = """
# PERGUNTAS CR√çTICAS PARA A JUNTA DE ESPECIALISTAS

## 1. AN√ÅLISE DE ARQUITETURA
- A arquitetura atual (Orchestrator ‚Üí Sequential Agents ‚Üí GitHub PR) √© adequada?
- Quais s√£o os principais gargalos arquiteturais?
- O padr√£o Committee + Fallback √© eficaz ou causa mais problemas?

## 2. WORKFLOW PROPOSTO
- Qual seria o workflow IDEAL para uma f√°brica de software com agentes IA?
- Devemos manter sequ√™ncia linear ou paralelizar agentes?
- Decomposer e Architect agregam valor ou s√£o overhead desnecess√°rio?

## 3. CR√çTICAS AO MODELO ATUAL
- Por que o Planner consistentemente falha em gerar JSON v√°lido?
- √â vi√°vel esperar que LLMs gerem arquivos completos em JSON?
- O Orchestrator Sentry com 6 retries √© estrat√©gia correta?

## 4. ESTRAT√âGIAS DE MELHORIA
- Como resolver definitivamente o problema do Planner?
- JSON Sanitizer √© solu√ß√£o ou workaround que mascara problema maior?
- Devemos simplificar o formato de sa√≠da (ex: gerar arquivos .py direto)?

## 5. INTERFACE DE USU√ÅRIO (UI)
- A f√°brica FlashSoft precisa de uma UI web?
- Que funcionalidades seriam essenciais na UI?
- Como integrar UI com pipeline CLI existente?
- Frameworks recomendados (Streamlit, Gradio, FastAPI+React, outros)?

## 6. RECOMENDA√á√ïES IMEDIATAS
- Qual √© a a√ß√£o MAIS URGENTE para conseguir 1 execu√ß√£o completa bem-sucedida?
- Devemos descartar c√≥digo atual e recome√ßar, ou continuar iterando?
- H√° exemplos de projetos similares (agent orchestration) que devemos estudar?

RESPONDA CADA SE√á√ÉO COM M√ÅXIMO DETALHE E FUNDAMENTA√á√ÉO T√âCNICA.
"""

def consult_model(model_key: str, model_info: dict):
    """Consulta um modelo espec√≠fico e salva relat√≥rio"""
    model_id = model_info['id']
    model_name = model_info['name']
    
    print(f"\n{'='*80}")
    print(f"ü§ñ CONSULTANDO: {model_name}")
    print(f"   Model ID: {model_id}")
    print(f"{'='*80}\n")
    
    start_time = time.time()
    
    system_prompt = """You are a senior software architect and AI systems expert with deep knowledge of:
- LLM orchestration and agent-based systems
- Software factory automation
- JSON parsing and structured output from LLMs
- CI/CD pipelines and GitHub automation
- Python architecture and design patterns
- UI/UX for developer tools

Provide detailed, technical, and actionable analysis. Be critical but constructive."""

    user_prompt = f"{CONTEXT}\n\n{QUESTIONS}"
    
    try:
        print(f"‚è≥ Enviando prompt ({len(user_prompt)} chars)...")
        
        # CORRE√á√ÉO: chat() espera system e user como args separados, n√£o messages list
        response = chat(
            model=model_id,
            system=system_prompt,
            user=user_prompt,
            max_tokens=4000,
            temperature=0.3
        )
        
        elapsed = time.time() - start_time
        
        # Calcular tokens aproximados
        input_tokens = (len(system_prompt) + len(user_prompt)) // 4
        output_tokens = len(response) // 4
        
        print(f"‚úì Resposta recebida!")
        print(f"  Tempo: {elapsed:.1f}s")
        print(f"  Input tokens: ~{input_tokens}")
        print(f"  Output tokens: ~{output_tokens}")
        print(f"  Total tokens: ~{input_tokens + output_tokens}")
        
        # Salvar relat√≥rio
        report = {
            'model': model_name,
            'model_id': model_id,
            'timestamp': datetime.now().isoformat(),
            'elapsed_seconds': round(elapsed, 2),
            'tokens': {
                'input_approximate': input_tokens,
                'output_approximate': output_tokens,
                'total_approximate': input_tokens + output_tokens
            },
            'response': response
        }
        
        # Salvar JSON
        json_filename = f"logs/consulta_{model_key}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"  Salvo: {json_filename}")
        
        # Salvar texto leg√≠vel
        txt_filename = f"logs/consulta_{model_key}.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"CONSULTA √Ä JUNTA DE ESPECIALISTAS\n")
            f.write(f"{'='*80}\n\n")
            f.write(f"Modelo: {model_name} ({model_id})\n")
            f.write(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Tempo: {elapsed:.1f}s\n")
            f.write(f"Input tokens: ~{input_tokens}\n")
            f.write(f"Output tokens: ~{output_tokens}\n")
            f.write(f"Total tokens: ~{input_tokens + output_tokens}\n\n")
            f.write(f"{'='*80}\n")
            f.write(f"RESPOSTA:\n")
            f.write(f"{'='*80}\n\n")
            f.write(response)
        print(f"  Salvo: {txt_filename}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO: {e}")
        
        # Salvar erro
        error_filename = f"logs/consulta_{model_key}_ERROR.txt"
        with open(error_filename, 'w', encoding='utf-8') as f:
            f.write(f"ERRO NA CONSULTA\n")
            f.write(f"Modelo: {model_name} ({model_id})\n")
            f.write(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"Erro: {str(e)}\n")
        
        return False

def main():
    print("\n" + "="*80)
    print("CONSULTA COMPLETA √Ä JUNTA DE ESPECIALISTAS - FLASHSOFT")
    print("="*80)
    print(f"\nModelos a consultar:")
    for key, info in MODELS.items():
        print(f"  ‚Ä¢ {info['name']} ({info['id']})")
    print("\n")
    
    results = {}
    
    for model_key, model_info in MODELS.items():
        success = consult_model(model_key, model_info)
        results[model_key] = success
        
        # Pausa entre consultas
        if model_key != list(MODELS.keys())[-1]:
            print("\n‚è∏  Aguardando 3s antes da pr√≥xima consulta...")
            time.sleep(3)
    
    # Resumo final
    print(f"\n\n{'='*80}")
    print("RESUMO DA CONSULTA")
    print(f"{'='*80}\n")
    
    for model_key, success in results.items():
        model_name = MODELS[model_key]['name']
        status = "‚úì SUCESSO" if success else "‚úó FALHA"
        print(f"  {status}: {model_name}")
    
    successful = sum(1 for s in results.values() if s)
    print(f"\n  Total: {successful}/{len(MODELS)} consultas bem-sucedidas")
    
    print(f"\n{'='*80}")
    print("Relat√≥rios salvos em:")
    print("  - logs/consulta_gemini25pro.json + .txt")
    print("  - logs/consulta_gpt5codex.json + .txt")
    print("  - logs/consulta_grok4.json + .txt")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()
